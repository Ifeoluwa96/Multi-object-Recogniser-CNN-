{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"project3 animal(CNN).ipynb","provenance":[],"mount_file_id":"1-0L_6dVTpXW4_8plNm6eL4KXckiU0CZT","authorship_tag":"ABX9TyPhqiFW8dXpGvGIk7Y42TUU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tuDvN9Vpp-WH","executionInfo":{"elapsed":16903,"status":"ok","timestamp":1639230819214,"user":{"displayName":"Ifeoluwa Wuraola","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-DwvCiL3diJgMX74jeDMoAVLvyIMYarcw0JDw=s64","userId":"00383867626927664331"},"user_tz":0},"outputId":"b3b63b12-d267-4f27-ac32-d0716dd56760"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"WYjazPPBqUk9"},"source":["# DEEP LEARNING\n","## Convolutionary Neural Network"]},{"cell_type":"code","metadata":{"id":"-KMRmD7lp_59"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import PIL\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","import pathlib\n","\n","data_dir = \"/content/drive/MyDrive/animal_photos\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cwipm52qaar"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlnQO45UrN9g","executionInfo":{"elapsed":8728,"status":"ok","timestamp":1638725597274,"user":{"displayName":"Ifeoluwa Wuraola","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-DwvCiL3diJgMX74jeDMoAVLvyIMYarcw0JDw=s64","userId":"00383867626927664331"},"user_tz":0},"outputId":"39bc3256-5676-4ce4-fae1-8b5f914384f4"},"source":["batch_size = 32\n","img_height = 180\n","img_width = 180\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split = 0.2,\n","    subset = \"training\",\n","    seed = 123,\n","    image_size = (img_height,img_width),\n","    batch_size = batch_size\n",")\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split = 0.2,\n","    subset = \"validation\",\n","    seed = 123,\n","    image_size = (img_height,img_width),\n","    batch_size = batch_size\n",")\n","\n","class_names = train_ds.class_names\n","print(class_names)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2423 files belonging to 3 classes.\n","Using 1939 files for training.\n","Found 2423 files belonging to 3 classes.\n","Using 484 files for validation.\n","['bear', 'horse', 'zebra']\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"w7LBqSZ9p_86","outputId":"e45edbdb-d67e-4654-a386-b40db8a4f09f"},"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)\n","\n","normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n","\n","normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","image_batch, labels_batch = next(iter(normalized_ds))\n","first_image = image_batch[0]\n","#Notice the pixels values are now in '[0,1]'.\n","print(np.min(first_image), np.max(first_image))\n","\n","num_classes = 3"],"execution_count":null,"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0df5d2840e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnormalized_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnormalization_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfirst_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Notice the pixels values are now in '[0,1]'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2840\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2841\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2843\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"rlfLLfhdp__i"},"source":["model = Sequential([\n","    layers.experimental.preprocessing.Rescaling(1./255, input_shape = (img_height,img_width, 3)),\n","    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n","    layers.MaxPooling2D(),\n","    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n","    layers.MaxPooling2D(),\n","    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n","    layers.MaxPooling2D(),\n","    layers.Flatten(),\n","    layers.Dense(128, activation = 'relu'),\n","    layers.Dense(num_classes)\n","])\n","\n","\n","model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n","              optimizer = 'adam', \n","              metrics = ['accuracy'])\n","\n","model.summary() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"0sMj-ypxqAFH"},"source":["epochs=10\n","history = model.fit(train_ds, \n","                    validation_data = val_ds, \n","                    epochs = epochs )\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"EG2_tiBmqAII"},"source":["plt.figure(figsize=(8, 8)) \n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy') \n","plt.plot(epochs_range, val_acc, label='Validation Accuracy') \n","plt.legend(loc='lower right') \n","plt.title('Training and Validation Accuracy') \n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss') \n","plt.plot(epochs_range, val_loss, label='Validation Loss') \n","plt.legend(loc='upper right') \n","plt.title('Training and Validation Loss') \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"Eek7FmkNqAK_"},"source":["plt.figure(figsize = (10,10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype('uint8'))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")\n","\n","for image_batch, labels_batch in train_ds:\n","  print(image_batch.shape)\n","  print(labels_batch.shape)\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5DFZXDrxzz3"},"source":["# Use data augmentation\n","\n"," — As our dataset is small and some of the results probably not as good \n","as we had hoped, it can be a good idea to use data augmentation - a process that adds random \n","but realistic variation to some of our samples, e.g. rotating them or flipping them horizontally."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"EEQdhXBvqANF"},"source":["data_augmentation = tf.keras.Sequential([\n","     tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n","     tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),                                 \n","])\n","\n","for image, _ in train_ds.take(1):\n","  plt.figure(figsize=(10, 10))\n","  first_image = image[0]\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n","    plt.imshow(augmented_image[0] / 255)\n","    plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"bF_ckL3TzUuK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jK7ZDcLSzpXC"},"source":["Let’s explore some other ideas too.\n","# Using feature extraction from pre-trained models"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"4RfyyniTzUxj"},"source":["# create the base model from the pre-trained model MoblieNet V2\n","normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n","\n","normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","image_batch, labels_batch = next(iter(normalized_ds))\n","first_image = image_batch[0]\n","# Notice the pixels values are now in `[0,1]`.\n","print(np.min(first_image), np.max(first_image))\n","\n","num_classes = 3\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"p2ZWmZwwzU0q"},"source":["model = Sequential([\n","  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n","  layers.Conv2D(16, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(32, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(64, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Flatten(),\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(num_classes)\n","])\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"slVqyIoszU3T"},"source":["epochs=10\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"G9veJzBszU6D"},"source":["plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"bKSc3vYbzU9H"},"source":["plt.figure(figsize = (10,10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype('uint8'))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")\n","\n","for image_batch, labels_batch in train_ds:\n","  print(image_batch.shape)\n","  print(labels_batch.shape)\n","  break"],"execution_count":null,"outputs":[]}]}